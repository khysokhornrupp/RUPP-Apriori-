{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from functools import reduce\n",
    "from collections import Iterable\n",
    "\n",
    "# use for vertify \n",
    "from mlxtend.frequent_patterns import apriori, association_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_dataset_path = \"/home/sokhorn/sokhorn/dataSet/data/Online Retail.csv\"\n",
    "testing_dataset_path = '/home/sokhorn/sokhorn/dataSet/data/sample_data_set.csv'\n",
    "sample_dataset = pd.read_csv(\n",
    "    real_dataset_path, sep=',', usecols=[\n",
    "        'InvoiceNo',\n",
    "        'StockCode',\n",
    "        'Quantity'\n",
    "    ])\n",
    "sample_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_sets = (\n",
    "    sample_dataset.groupby(['InvoiceNo', 'StockCode', ])['Quantity']\n",
    "    .sum().unstack().reset_index().fillna(0)\n",
    "    .set_index(\"InvoiceNo\")\n",
    ")\n",
    "item_sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_code = lambda x : 1 if x > 0  else 0\n",
    "item_sets = item_sets.applymap(en_code)\n",
    "item_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating RC column for each tranctions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rc(item_np):\n",
    "    np_hash = {}\n",
    "    for item in item_np:\n",
    "        key = \" \".join(map(str, item))\n",
    "        if np_hash.get(key):\n",
    "            np_hash[key] += 1\n",
    "        else:\n",
    "            np_hash[key] = 1\n",
    "    item_rc = []\n",
    "    for item in np_hash:\n",
    "        values =  list(map(int,  item.split()))\n",
    "        values.append(np_hash[item])\n",
    "        item_rc.append(values)\n",
    "    return item_rc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rc =  pd.DataFrame(np.array(rc(item_sets.values)))\n",
    "columns = item_sets.columns\n",
    "columns.append(\"RC\")\n",
    "df_rc.set_axis(columns, inplace=True  , axis=1 )\n",
    "RC = df_rc['RC']\n",
    "df_rc.drop(['RC'], axis=1, inplace=True)\n",
    "df_rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def support_k_itemst(k_itemst):\n",
    "    s = 0\n",
    "    for i in range(len(k_itemst)):\n",
    "        s += reduce(lambda a, b: a & b, k_itemst[i] & RC[i])\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ocurence(itemset, Tranctions):\n",
    "    count = 0\n",
    "    for i in range(len(Tranctions)):\n",
    "        if set(itemset).issubset(set(Tranctions[i])):\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_two_itemsets(it1, it2, order):\n",
    "    it1.sort(key=lambda x : order.index(x))\n",
    "    it2.sort(key=lambda x : order.index(x))\n",
    "\n",
    "    for i in range(len(it1) - 1): # check befor the last one \n",
    "       if it1[i] != it2 [i] :\n",
    "           return []\n",
    "    \n",
    "    if order.index(it1[-1]) < order.index(it2[-1]) :\n",
    "        return [it1] + [it2[-1]]\n",
    "\n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeKItemIntoOne(ab):\n",
    "    result = []\n",
    "    if(len(ab) != 0):\n",
    "        if(len(ab[0]) == 1):\n",
    "            return ab\n",
    "        else:\n",
    "            for item in ab:\n",
    "                res = item[0] + item[1].split()\n",
    "                result.append(res)\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequent(itemesets, min_support, prev_discard):\n",
    "    L = []\n",
    "    support_count = []\n",
    "    new_discard = []\n",
    "    column_items = mergeKItemIntoOne(itemesets)\n",
    "\n",
    "    k = len(prev_discard)\n",
    "    for i in range(len(itemesets)):\n",
    "        discard_before = False\n",
    "        item = itemesets[i] \n",
    "        result = []\n",
    "        for i_item in item:\n",
    "            if isinstance(i_item, list):\n",
    "                for j in i_item:\n",
    "                    result.append(j)\n",
    "            else:\n",
    "                result.append(i_item)\n",
    "\n",
    "        if k > 0:\n",
    "            for it in prev_discard[k]:\n",
    "                if set(it).issubset(set(result)):\n",
    "                    discard_before = True\n",
    "                    break\n",
    "        \n",
    "        if not discard_before:\n",
    "            # print(f'item {mergeKItemIntoOne(item)}')\n",
    "            count = support_k_itemst(df_rc[column_items[i]].values)\n",
    "            count = 1\n",
    "            if count >= min_support:\n",
    "                L.append(result)\n",
    "                support_count.append(count)\n",
    "            else:\n",
    "                new_discard.append(result)\n",
    "    return L, support_count, new_discard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_set_item(set_of_its, order_column_name):\n",
    "    C = []\n",
    "    for i in range(len(set_of_its)):\n",
    "        for j in range(i + 1, len(set_of_its)):\n",
    "            it_out = join_two_itemsets(\n",
    "                set_of_its[i], set_of_its[j], order_column_name)\n",
    "            if(len(it_out)) > 0:\n",
    "                C.append(it_out)\n",
    "\n",
    "    return C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating 1 itemse base on support count of row**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = {}\n",
    "L = {}\n",
    "k_items = []\n",
    "Discard = {}\n",
    "itemset_size = 1\n",
    "min_support = 1\n",
    "Discard.update({itemset_size : []})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating 1 itemset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove each column who support count are less than min_support for 1 itemsets\n",
    "rc_1_itemset = df_rc.sum(axis=0)\n",
    "cut_our_cols = rc_1_itemset.loc[lambda x: x < min_support].index\n",
    "cut_our_cols\n",
    "df_rc.drop(labels=cut_our_cols, axis=1, inplace=True)\n",
    "df_rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C.update({itemset_size: np.reshape(list(df_rc.columns), (-1, 1))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_count = {}\n",
    "f, supp, new_discard = get_frequent(\n",
    "    C[itemset_size], min_support, Discard\n",
    ")\n",
    "Discard.update({itemset_size: new_discard})\n",
    "L.update({itemset_size: f})\n",
    "support_count.update({itemset_size: supp})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = itemset_size + 1\n",
    "convergence = False\n",
    "while not convergence:\n",
    "    C.update({k: join_set_item(L[k - 1], list( df_rc.columns))})\n",
    "    f, supp, new_discard = get_frequent(C[k], min_support, Discard)\n",
    "    L.update({k: f})\n",
    "    Discard.update({k: new_discard})\n",
    "    support_count.update({k: supp})\n",
    "    if(len(L[k]) == 0):\n",
    "        convergence = True\n",
    "    k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemset = apriori(item_sets, min_support=min_support / len(item_sets), use_colnames=True)\n",
    "itemset"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
