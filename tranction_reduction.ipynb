{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy_indexed as npi\n",
    "from itertools import combinations, product\n",
    "from functools import reduce\n",
    "from collections.abc import Iterable\n",
    "import math\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from mlxtend.frequent_patterns import association_rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_dataset_path = \"/home/sokhorn/sokhorn/dataSet/data/Online Retail.csv\"\n",
    "testing_dataset_path = '/home/sokhorn/sokhorn/dataSet/data/sample_data_set.csv'\n",
    "tranction_reduction_itemsets = '/home/sokhorn/sokhorn/dataSet/data/sample_tranc_red.csv'\n",
    "sample_dataset = pd.read_csv(\n",
    "    real_dataset_path, sep=',', usecols=[\n",
    "        'InvoiceNo',\n",
    "        'StockCode',\n",
    "        'Quantity', \"Country\"\n",
    "    ])\n",
    "\n",
    "# item_sets = (\n",
    "#     sample_dataset[sample_dataset['Country'] == 'France'].groupby(['InvoiceNo', 'StockCode', ])['Quantity']\n",
    "#     .sum().unstack().reset_index().fillna(0)\n",
    "#     .set_index(\"InvoiceNo\")\n",
    "# )\n",
    "# item_sets = (\n",
    "#     sample_dataset.groupby(['InvoiceNo', 'StockCode', ])['Quantity']\n",
    "#     .sum().unstack().reset_index().fillna(0)\n",
    "#     .set_index(\"InvoiceNo\")\n",
    "# )\n",
    "# item_sets = item_sets.applymap(lambda x: 1 if x > 0 else 0)\n",
    "# item_sets.reindex(sorted(item_sets.columns), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>InvoiceNo</th>\n",
       "      <th>StockCode</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>536365</td>\n",
       "      <td>85123A</td>\n",
       "      <td>6</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>536365</td>\n",
       "      <td>71053</td>\n",
       "      <td>6</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>536365</td>\n",
       "      <td>84406B</td>\n",
       "      <td>8</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>536365</td>\n",
       "      <td>84029G</td>\n",
       "      <td>6</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>536365</td>\n",
       "      <td>84029E</td>\n",
       "      <td>6</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541918</th>\n",
       "      <td>581597</td>\n",
       "      <td>23256</td>\n",
       "      <td>4</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541919</th>\n",
       "      <td>581598</td>\n",
       "      <td>22613</td>\n",
       "      <td>12</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541920</th>\n",
       "      <td>581599</td>\n",
       "      <td>22899</td>\n",
       "      <td>6</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541921</th>\n",
       "      <td>581600</td>\n",
       "      <td>23254</td>\n",
       "      <td>4</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541922</th>\n",
       "      <td>581601</td>\n",
       "      <td>23255</td>\n",
       "      <td>4</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>541923 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       InvoiceNo StockCode  Quantity         Country\n",
       "0         536365    85123A         6  United Kingdom\n",
       "1         536365     71053         6  United Kingdom\n",
       "2         536365    84406B         8  United Kingdom\n",
       "3         536365    84029G         6  United Kingdom\n",
       "4         536365    84029E         6  United Kingdom\n",
       "...          ...       ...       ...             ...\n",
       "541918    581597     23256         4          France\n",
       "541919    581598     22613        12          France\n",
       "541920    581599     22899         6          France\n",
       "541921    581600     23254         4          France\n",
       "541922    581601     23255         4          France\n",
       "\n",
       "[541923 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset.head(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def itemsett_by_country(country):\n",
    "    item_sets_country = (\n",
    "        sample_dataset[sample_dataset['Country'] == country].groupby(\n",
    "            ['InvoiceNo', 'StockCode', ])['Quantity']\n",
    "        .sum().unstack().reset_index().fillna(0)\n",
    "        .set_index(\"InvoiceNo\")\n",
    "    )\n",
    "    item_sets_country = item_sets_country.applymap(lambda x: 1 if x > 0 else 0)\n",
    "    item_sets_country.reindex(sorted(item_sets_country.columns), axis=1)\n",
    "    return item_sets_country\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_sets = itemsett_by_country(\"France\")\n",
    "item_sets = item_sets.iloc[0: 10]\n",
    "item_sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_minsupp(item_sets):\n",
    "    minimum_support_count = (60 / 100) * len(item_sets)\n",
    "    return minimum_support_count\n",
    "\n",
    "\n",
    "user_minsupp(item_sets) / len(item_sets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tranditonal Apriori Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sebastian Raschka 2014-2020\n",
    "# myxtend Machine Learning Library Extensions\n",
    "# Author: Sebastian Raschka <sebastianraschka.com>\n",
    "#\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def generate_new_combinations(old_combinations):\n",
    "    items_types_in_previous_step = np.unique(old_combinations.flatten())\n",
    "    for old_combination in old_combinations:\n",
    "        max_combination = old_combination[-1]  # get a single item in the last\n",
    "        mask = items_types_in_previous_step > max_combination\n",
    "        valid_items = items_types_in_previous_step[mask]\n",
    "        old_tuple = tuple(old_combination)\n",
    "        for item in valid_items:\n",
    "\n",
    "            yield from old_tuple\n",
    "            yield item\n",
    "\n",
    "\n",
    "def generate_new_combinations_low_memory(old_combinations, X, min_support,\n",
    "                                         is_sparse):\n",
    "\n",
    "    items_types_in_previous_step = np.unique(old_combinations.flatten())\n",
    "    rows_count = X.shape[0]\n",
    "    threshold = min_support * rows_count\n",
    "    for old_combination in old_combinations:\n",
    "        max_combination = old_combination[-1]\n",
    "        mask = items_types_in_previous_step > max_combination\n",
    "        valid_items = items_types_in_previous_step[mask]\n",
    "        old_tuple = tuple(old_combination)\n",
    "        if is_sparse:\n",
    "            mask_rows = X[:, old_tuple].toarray().all(axis=1)\n",
    "            X_cols = X[:, valid_items].toarray()\n",
    "            supports = X_cols[mask_rows].sum(axis=0)\n",
    "        else:\n",
    "            mask_rows = X[:, old_tuple].all(axis=1)\n",
    "            supports = X[mask_rows][:, valid_items].sum(axis=0)\n",
    "        valid_indices = (supports >= threshold).nonzero()[0]\n",
    "        for index in valid_indices:\n",
    "            yield supports[index]\n",
    "            yield from old_tuple\n",
    "            yield valid_items[index]\n",
    "\n",
    "\n",
    "def apriori(df, min_support=0.5, use_colnames=False, max_len=None, verbose=0,\n",
    "            low_memory=False):\n",
    "    start = timeit.default_timer()\n",
    "\n",
    "    def _support(_x, _n_rows, _is_sparse):\n",
    "\n",
    "        out = (np.sum(_x, axis=0) / _n_rows)\n",
    "        return np.array(out).reshape(-1)\n",
    "\n",
    "    if min_support <= 0.:\n",
    "        raise ValueError('`min_support` must be a positive '\n",
    "                         'number within the interval `(0, 1]`. '\n",
    "                         'Got %s.' % min_support)\n",
    "\n",
    "    if hasattr(df, \"sparse\"):\n",
    "        # DataFrame with SparseArray (pandas >= 0.24)\n",
    "        if df.size == 0:\n",
    "            X = df.values\n",
    "        else:\n",
    "            X = df.sparse.to_coo().tocsc()\n",
    "        is_sparse = True\n",
    "    else:\n",
    "        # dense DataFrame\n",
    "        X = df.values\n",
    "        is_sparse = False\n",
    "    support = _support(X, X.shape[0], is_sparse)\n",
    "    ary_col_idx = np.arange(X.shape[1])\n",
    "    support_dict = {1: support[support >= min_support]}\n",
    "    itemset_dict = {1: ary_col_idx[support >= min_support].reshape(-1, 1)}\n",
    "    max_itemset = 1\n",
    "    rows_count = float(X.shape[0])\n",
    "\n",
    "    all_ones = np.ones((int(rows_count), 1))\n",
    "\n",
    "    while max_itemset and max_itemset < (max_len or float('inf')):\n",
    "        next_max_itemset = max_itemset + 1\n",
    "\n",
    "        # With exceptionally large datasets, the matrix operations can use a\n",
    "        # substantial amount of memory. For low memory applications or large\n",
    "        # datasets, set `low_memory=True` to use a slower but more memory-\n",
    "        # efficient implementation.\n",
    "        if low_memory:\n",
    "            combin = generate_new_combinations_low_memory(\n",
    "                itemset_dict[max_itemset], X, min_support, is_sparse)\n",
    "            # slightly faster than creating an array from a list of tuples\n",
    "            combin = np.fromiter(combin, dtype=int)\n",
    "            combin = combin.reshape(-1, next_max_itemset + 1)\n",
    "\n",
    "            if combin.size == 0:\n",
    "                break\n",
    "            if verbose:\n",
    "                print(\n",
    "                    '\\rProcessing %d combinations | Sampling itemset size %d' %\n",
    "                    (combin.size, next_max_itemset), end=\"\")\n",
    "\n",
    "            itemset_dict[next_max_itemset] = combin[:, 1:]\n",
    "            support_dict[next_max_itemset] = combin[:, 0].astype(float) \\\n",
    "                / rows_count\n",
    "            max_itemset = next_max_itemset\n",
    "        else:\n",
    "            # conver from generator to numpy\n",
    "            combin = generate_new_combinations(itemset_dict[max_itemset])\n",
    "            combin = np.fromiter(combin, dtype=int)\n",
    "            combin = combin.reshape(-1, next_max_itemset)\n",
    "\n",
    "            # end generator\n",
    "            if combin.size == 0:\n",
    "                break\n",
    "            if verbose:\n",
    "                print(\n",
    "                    '\\rProcessing %d combinations | Sampling itemset size %d' %\n",
    "                    (combin.size, next_max_itemset), end=\"\")\n",
    "\n",
    "            if is_sparse:\n",
    "                _bools = X[:, combin[:, 0]] == all_ones\n",
    "                for n in range(1, combin.shape[1]):\n",
    "                    _bools = _bools & (X[:, combin[:, n]] == all_ones)\n",
    "            else:\n",
    "                _bools = np.all(X[:, combin], axis=2)\n",
    "\n",
    "            support = _support(np.array(_bools), rows_count, is_sparse)\n",
    "\n",
    "            _mask = (support >= min_support).reshape(-1)\n",
    "\n",
    "            if any(_mask):\n",
    "\n",
    "                itemset_dict[next_max_itemset] = np.array(combin[_mask])\n",
    "                support_dict[next_max_itemset] = np.array(support[_mask])\n",
    "                max_itemset = next_max_itemset\n",
    "\n",
    "            else:\n",
    "                # Exit condition\n",
    "                # when there no more itemst\n",
    "                break\n",
    "\n",
    "    all_res = []\n",
    "    for k in sorted(itemset_dict):\n",
    "        support = pd.Series(support_dict[k])\n",
    "        itemsets = pd.Series([frozenset(i) for i in itemset_dict[k]],\n",
    "                             dtype='object')\n",
    "\n",
    "        res = pd.concat((support, itemsets), axis=1)\n",
    "        all_res.append(res)\n",
    "\n",
    "    res_df = pd.concat(all_res)\n",
    "    res_df.columns = ['support', 'itemsets']\n",
    "    if use_colnames:\n",
    "        mapping = {idx: item for idx, item in enumerate(df.columns)}\n",
    "        res_df['itemsets'] = res_df['itemsets'].apply(lambda x: frozenset([\n",
    "                                                      mapping[i] for i in x]))\n",
    "    res_df = res_df.reset_index(drop=True)\n",
    "\n",
    "    if verbose:\n",
    "        print()  # adds newline if verbose counter was used\n",
    "    stop = timeit.default_timer()\n",
    "    execution_time = stop - start\n",
    "    # print(f\"execution time : {execution_time}\")\n",
    "    return res_df, execution_time\n",
    "\n",
    "\n",
    "print(\"Apriori Itemsets\")\n",
    "fre_apriori_itemsets = apriori(item_sets, min_support=0.4, use_colnames=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tranction Reduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RC count generation\n",
    "itemset_reductabc, rc_values = npi.count(item_sets.values)\n",
    "itemset_reductabc = pd.DataFrame(itemset_reductabc, columns=item_sets.columns)\n",
    "itemset_reductabc = itemset_reductabc.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sebastian Raschka 2014-2020\n",
    "# myxtend Machine Learning Library Extensions\n",
    "# Author: Sebastian Raschka <sebastianraschka.com>\n",
    "#\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# RC count generation\n",
    "itemset_reductabc, rc_values = npi.count(item_sets.values)\n",
    "itemset_reductabc = pd.DataFrame(itemset_reductabc, columns=item_sets.columns)\n",
    "itemset_reductabc = itemset_reductabc.astype(int)\n",
    "\n",
    "\n",
    "def generate_new_combinations(old_combinations):\n",
    "\n",
    "    items_types_in_previous_step = np.unique(old_combinations.flatten())\n",
    "    for old_combination in old_combinations:\n",
    "        max_combination = old_combination[-1]  # get a single item in the last\n",
    "        mask = items_types_in_previous_step > max_combination\n",
    "        valid_items = items_types_in_previous_step[mask]\n",
    "\n",
    "        old_tuple = tuple(old_combination)\n",
    "        for item in valid_items:\n",
    "            yield from old_tuple\n",
    "            yield item\n",
    "\n",
    "\n",
    "def generate_new_combinations_low_memory(old_combinations, X, min_support,\n",
    "                                         is_sparse):\n",
    "\n",
    "    items_types_in_previous_step = np.unique(old_combinations.flatten())\n",
    "    rows_count = X.shape[0]\n",
    "    threshold = min_support * rows_count\n",
    "    for old_combination in old_combinations:\n",
    "        max_combination = old_combination[-1]\n",
    "        mask = items_types_in_previous_step > max_combination\n",
    "        valid_items = items_types_in_previous_step[mask]\n",
    "        old_tuple = tuple(old_combination)\n",
    "        if is_sparse:\n",
    "            mask_rows = X[:, old_tuple].toarray().all(axis=1)\n",
    "            X_cols = X[:, valid_items].toarray()\n",
    "            supports = X_cols[mask_rows].sum(axis=0)\n",
    "        else:\n",
    "            mask_rows = X[:, old_tuple].all(axis=1)\n",
    "            supports = X[mask_rows][:, valid_items].sum(axis=0)\n",
    "        valid_indices = (supports >= threshold).nonzero()[0]\n",
    "        for index in valid_indices:\n",
    "            yield supports[index]\n",
    "            yield from old_tuple\n",
    "            yield valid_items[index]\n",
    "\n",
    "\n",
    "def apriori_tranction_reduction(df, min_support=0.5, use_colnames=False, max_len=None, rc_values=None, verbose=0):\n",
    "    start = timeit.default_timer()\n",
    "\n",
    "    def _tranction_reduc_support(k_itemset, rc_values, tranction_size):\n",
    "        print(f\"Tranction Size {tranction_size}\")\n",
    "        return ((np.bitwise_and.reduce(k_itemset, axis=2) * rc_values.reshape(-1, 1)).sum(axis=0) / tranction_size)\n",
    "\n",
    "    # def fre_1_itemset(item, minsupp=0):\n",
    "    #     print(f\"Min Support : {minsupp}\")\n",
    "    #     rc_of_1_itemset = item.sum(axis=0)\n",
    "    #     item_index = np.arange(item.shape[1])\n",
    "    #     return rc_of_1_itemset.values[rc_of_1_itemset.values > minsupp], item_index[rc_of_1_itemset.values > minsupp]\n",
    "\n",
    "    def fre_1_itemset(item, minsupp=0):\n",
    "        rc_of_1_itemset = item.sum(axis=0)\n",
    "        tranction_size = len(item)\n",
    "        item_index = np.arange(item.shape[1])\n",
    "        mask = (rc_of_1_itemset.values / tranction_size) >= minsupp\n",
    "        return rc_of_1_itemset.values[mask] / tranction_size, item_index[mask]\n",
    "\n",
    "    if min_support <= 0.:\n",
    "        raise ValueError('`min_support` must be a positive '\n",
    "                         'number within the interval `(0, 1]`. '\n",
    "                         'Got %s.' % min_support)\n",
    "\n",
    "    if hasattr(df, \"sparse\"):\n",
    "        # DataFrame with SparseArray (pandas >= 0.24)\n",
    "        if df.size == 0:\n",
    "            X = df.values\n",
    "        else:\n",
    "            X = df.sparse.to_coo().tocsc()\n",
    "    else:\n",
    "        # dense DataFrame\n",
    "        X = df.values\n",
    "    tranction_size = len(X)\n",
    "    one_itemset, support_one_itemset = fre_1_itemset(\n",
    "        df, minsupp=min_support)\n",
    "\n",
    "    support_dict = {1: one_itemset}\n",
    "    itemset_dict = {1: support_one_itemset.reshape(-1, 1)}\n",
    "    max_itemset = 1\n",
    "\n",
    "    while max_itemset and max_itemset < (max_len or float('inf')):\n",
    "\n",
    "        next_max_itemset = max_itemset + 1\n",
    "        # convert from generator to numpy\n",
    "        combin = generate_new_combinations(itemset_dict[max_itemset])\n",
    "        combin = np.fromiter(combin, dtype=int)\n",
    "        combin = combin.reshape(-1, next_max_itemset)\n",
    "\n",
    "        if combin.size == 0:  # No more itemset to generate\n",
    "            break\n",
    "\n",
    "        support = _tranction_reduc_support(\n",
    "            X[:, combin], rc_values, tranction_size\n",
    "        )\n",
    "        _mask = (support >= min_support).reshape(-1)\n",
    "        if any(_mask):\n",
    "            # this will be generate item those are frequent\n",
    "            itemset_dict[next_max_itemset] = np.array(combin[_mask])\n",
    "            support_dict[next_max_itemset] = np.array(support[_mask])\n",
    "            max_itemset = next_max_itemset\n",
    "        else:\n",
    "            # Exit condition\n",
    "            # when there no more itemst\n",
    "            break\n",
    "\n",
    "    all_res = []\n",
    "    for k in sorted(itemset_dict):\n",
    "        support = pd.Series(support_dict[k])\n",
    "        itemsets = pd.Series([frozenset(i) for i in itemset_dict[k]],\n",
    "                             dtype='object')\n",
    "        res = pd.concat((support, itemsets), axis=1)\n",
    "        all_res.append(res)\n",
    "\n",
    "    res_df = pd.concat(all_res)\n",
    "    res_df.columns = ['support', 'itemsets']\n",
    "    if use_colnames:\n",
    "        mapping = {idx: item for idx, item in enumerate(df.columns)}\n",
    "        res_df['itemsets'] = res_df['itemsets'].apply(\n",
    "            lambda x: frozenset([mapping[i] for i in x])\n",
    "        )\n",
    "    res_df = res_df.reset_index(drop=True)\n",
    "    stop = timeit.default_timer()\n",
    "    execution_time = stop - start\n",
    "    # print(f\"execution time : {execution_time}\")\n",
    "    return res_df, execution_time\n",
    "\n",
    "\n",
    "print(\"Apriori Tranction Reduction Itemsets\")\n",
    "a, t = apriori_tranction_reduction(\n",
    "    itemset_reductabc, min_support=1, use_colnames=True, rc_values=rc_values)\n",
    "a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual Minimum support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_combinations(old_combinations):\n",
    "    items_types_in_previous_step = np.unique(old_combinations.flatten())\n",
    "    for old_combination in old_combinations:\n",
    "        max_combination = old_combination[-1]  # get a single item in the last\n",
    "        mask = items_types_in_previous_step > max_combination\n",
    "        valid_items = items_types_in_previous_step[mask]\n",
    "        old_tuple = tuple(old_combination)\n",
    "        for item in valid_items:\n",
    "\n",
    "            yield from old_tuple\n",
    "            yield item\n",
    "\n",
    "\n",
    "def generate_new_combinations_low_memory(old_combinations, X, min_support,\n",
    "                                         is_sparse):\n",
    "\n",
    "    items_types_in_previous_step = np.unique(old_combinations.flatten())\n",
    "    rows_count = X.shape[0]\n",
    "    threshold = min_support * rows_count\n",
    "    for old_combination in old_combinations:\n",
    "        max_combination = old_combination[-1]\n",
    "        mask = items_types_in_previous_step > max_combination\n",
    "        valid_items = items_types_in_previous_step[mask]\n",
    "        old_tuple = tuple(old_combination)\n",
    "        if is_sparse:\n",
    "            mask_rows = X[:, old_tuple].toarray().all(axis=1)\n",
    "            X_cols = X[:, valid_items].toarray()\n",
    "            supports = X_cols[mask_rows].sum(axis=0)\n",
    "        else:\n",
    "            mask_rows = X[:, old_tuple].all(axis=1)\n",
    "            supports = X[mask_rows][:, valid_items].sum(axis=0)\n",
    "        valid_indices = (supports >= threshold).nonzero()[0]\n",
    "        for index in valid_indices:\n",
    "            yield supports[index]\n",
    "            yield from old_tuple\n",
    "            yield valid_items[index]\n",
    "\n",
    "\n",
    "def actual_apriori(df, min_support=0, use_colnames=False, max_len=None, verbose=0,\n",
    "                   low_memory=False):\n",
    "    start = timeit.default_timer()\n",
    "\n",
    "    def _support(_x, _n_rows, _is_sparse):\n",
    "\n",
    "        out = (np.sum(_x, axis=0) / _n_rows)\n",
    "        return np.array(out).reshape(-1)\n",
    "\n",
    "    if hasattr(df, \"sparse\"):\n",
    "        # DataFrame with SparseArray (pandas >= 0.24)\n",
    "        if df.size == 0:\n",
    "            X = df.values\n",
    "        else:\n",
    "            X = df.sparse.to_coo().tocsc()\n",
    "        is_sparse = True\n",
    "    else:\n",
    "        # dense DataFrame\n",
    "        X = df.values\n",
    "        is_sparse = False\n",
    "    support = _support(X, X.shape[0], is_sparse)\n",
    "    ary_col_idx = np.arange(X.shape[1])\n",
    "    support_dict = {1: support[support > min_support]}\n",
    "    itemset_dict = {1: ary_col_idx[support > min_support].reshape(-1, 1)}\n",
    "    max_itemset = 1\n",
    "    rows_count = float(X.shape[0])\n",
    "\n",
    "    all_ones = np.ones((int(rows_count), 1))\n",
    "\n",
    "    while max_itemset and max_itemset < (max_len or float('inf')):\n",
    "        next_max_itemset = max_itemset + 1\n",
    "\n",
    "        # With exceptionally large datasets, the matrix operations can use a\n",
    "        # substantial amount of memory. For low memory applications or large\n",
    "        # datasets, set `low_memory=True` to use a slower but more memory-\n",
    "        # efficient implementation.\n",
    "        if low_memory:\n",
    "            combin = generate_new_combinations_low_memory(\n",
    "                itemset_dict[max_itemset], X, min_support, is_sparse)\n",
    "            # slightly faster than creating an array from a list of tuples\n",
    "            combin = np.fromiter(combin, dtype=int)\n",
    "            combin = combin.reshape(-1, next_max_itemset + 1)\n",
    "\n",
    "            if combin.size == 0:\n",
    "                break\n",
    "            if verbose:\n",
    "                print(\n",
    "                    '\\rProcessing %d combinations | Sampling itemset size %d' %\n",
    "                    (combin.size, next_max_itemset), end=\"\")\n",
    "\n",
    "            itemset_dict[next_max_itemset] = combin[:, 1:]\n",
    "            support_dict[next_max_itemset] = combin[:, 0].astype(float) \\\n",
    "                / rows_count\n",
    "            max_itemset = next_max_itemset\n",
    "        else:\n",
    "            # conver from generator to numpy\n",
    "            combin = generate_new_combinations(itemset_dict[max_itemset])\n",
    "            combin = np.fromiter(combin, dtype=int)\n",
    "            combin = combin.reshape(-1, next_max_itemset)\n",
    "\n",
    "            # end generator\n",
    "            if combin.size == 0:\n",
    "                break\n",
    "            if verbose:\n",
    "                print(\n",
    "                    '\\rProcessing %d combinations | Sampling itemset size %d' %\n",
    "                    (combin.size, next_max_itemset), end=\"\")\n",
    "\n",
    "            if is_sparse:\n",
    "                _bools = X[:, combin[:, 0]] == all_ones\n",
    "                for n in range(1, combin.shape[1]):\n",
    "                    _bools = _bools & (X[:, combin[:, n]] == all_ones)\n",
    "            else:\n",
    "                _bools = np.all(X[:, combin], axis=2)\n",
    "\n",
    "            support = _support(np.array(_bools), rows_count, is_sparse)\n",
    "\n",
    "            _mask = (support > min_support).reshape(-1)\n",
    "\n",
    "            if any(_mask):\n",
    "                itemset_dict[next_max_itemset] = np.array(combin[_mask])\n",
    "                support_dict[next_max_itemset] = np.array(support[_mask])\n",
    "                max_itemset = next_max_itemset\n",
    "            else:\n",
    "                # Exit condition\n",
    "                # when there no more itemst\n",
    "                break\n",
    "\n",
    "    all_res = []\n",
    "    for k in sorted(itemset_dict):\n",
    "        support = pd.Series(support_dict[k])\n",
    "        itemsets = pd.Series([frozenset(i) for i in itemset_dict[k]],\n",
    "                             dtype='object')\n",
    "        res = pd.concat((support, itemsets), axis=1)\n",
    "        all_res.append(res)\n",
    "\n",
    "    res_df = pd.concat(all_res)\n",
    "    res_df.columns = ['support', 'itemsets']\n",
    "    if use_colnames:\n",
    "        mapping = {idx: item for idx, item in enumerate(df.columns)}\n",
    "        res_df['itemsets'] = res_df['itemsets'].apply(lambda x: frozenset([\n",
    "                                                      mapping[i] for i in x]))\n",
    "    res_df = res_df.reset_index(drop=True)\n",
    "\n",
    "    if verbose:\n",
    "        print()  # adds newline if verbose counter was used\n",
    "    stop = timeit.default_timer()\n",
    "    execution_time = stop - start\n",
    "    print(f\"execution_time : {execution_time}\")\n",
    "    return res_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_frieq_itemset = actual_apriori(\n",
    "    item_sets, use_colnames=True, min_support=0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lean(supports, avg, m):\n",
    "    less_avg = np.apply_along_axis(\n",
    "        lambda x: x < avg, arr=supports, axis=0).sum()\n",
    "    greater_avg = np.apply_along_axis(\n",
    "        lambda x: x > avg, arr=supports, axis=0).sum()\n",
    "    return (less_avg - greater_avg) / m\n",
    "\n",
    "\n",
    "def aproximate_minsupport(r_min, min_supp, max_min, n=1):\n",
    "    a_n = pow(min_supp, n)\n",
    "    b_n = pow(max_min, n)\n",
    "    x_n = (r_min - (a_n / (a_n - b_n))) * (b_n - a_n)\n",
    "    return x_n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supports = act_frieq_itemset['support']\n",
    "a, b = supports.min(), supports.max()\n",
    "m = len(supports)\n",
    "avg = supports.sum() / m\n",
    "lean_value = lean(supports, m, avg)\n",
    "print(f\"Lean : {lean_value}\\nMinimum support : {a}\\nMaximum support : {b}\\nAverage : {avg}\\nTranction size : {m}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_b_m_avg_lean_actual_support(supports, user_min_support):\n",
    "    a, b = supports.min(), supports.max()\n",
    "    m = len(supports)\n",
    "    avg = supports.sum() / m\n",
    "    lean_value = lean(supports, m, avg)\n",
    "    act_min_support = aproximate_minsupport(user_min_support, a, b)\n",
    "    return a, b, avg, lean_value, act_min_support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apriximate minimum support with linear strategy\n",
    "aproximate_minsupport(0.2, a, b)\n",
    "# apriximate minimum support with Polynomial strategy\n",
    "aproximate_minsupport(0.2, a, b, n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori_actualMinimumSupport(min_support, data):\n",
    "    itemset_apriori = apriori(min_support=min_support, df=data)\n",
    "    act_min = a_b_m_avg_lean_actual_support(\n",
    "        supports=itemset_apriori['support'], user_min_support=min_support)[-1]\n",
    "    return itemset_apriori, act_min\n",
    "\n",
    "\n",
    "# apriori_itemset, act_minsupport = apriori_actualMinimumSupport(\n",
    "#     min_support=0.2, data=item_sets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def association_maxAntecedent_maxConsequents(fre_itemset, min_threshold=0.6):\n",
    "    start = timeit.default_timer()\n",
    "    rules = association_rules(\n",
    "        fre_itemset, metric='confidence', min_threshold=min_threshold)\n",
    "    max_antecedents = rules['antecedents'].str.len().max()\n",
    "    max_consequents = rules['consequents'].str.len().max()\n",
    "    stop = timeit.default_timer()\n",
    "    execution_time = stop - start\n",
    "    print(f\"execution time : {execution_time}\")\n",
    "    return rules, max_antecedents, max_consequents, execution_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Association Rule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Apriori\n",
    "# apriori_rules, maxAntecedentsAp, maxConsequentsAp, execution_time_Ap = association_maxAntecedent_maxConsequents(\n",
    "#     fre_apriori_itemsets, min_threshold=0.6)\n",
    "# tranction_redu_apriori_rules, maxAntecedentsTR, maxConsequentsTR, execution_time_TR = association_maxAntecedent_maxConsequents(\n",
    "#     freq_itemset_tranction_reduc, min_threshold=0.6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\n",
    "#     f\"Apriori\\nRule = {apriori_rules.shape[0]}\\nMinimum antecedents : {maxAntecedentsAp}\\nMaximum consequents : {maxConsequentsAp}\")\n",
    "# print(\n",
    "#     f\"\\nApriori Tranction Reduction\\nRule = {tranction_redu_apriori_rules.shape[0]}\\nMinimum antecedents : {maxAntecedentsTR}\\nMaximum consequents : {maxConsequentsTR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_can_recommend(antecedents, consequents):\n",
    "    antecedents = antecedents.str.len()\n",
    "    consequents = consequents.str.len()\n",
    "    print(f\"antecedents : {antecedents.max()}\")\n",
    "    ants = np.arange(1, antecedents.max())\n",
    "    result = {}\n",
    "    for i in ants:\n",
    "        index = consequents[antecedents > i]\n",
    "        item = set(index)\n",
    "        result.update({i: item})\n",
    "    return result\n",
    "\n",
    "\n",
    "# apriori_rules['consequents'][13616]\n",
    "# apriori_rules['antecedents'][13616]\n",
    "# print(\"Apriori :  When User Purchase : It can recommend up to\")\n",
    "# item_can_recommend(apriori_rules['antecedents'].str.len(\n",
    "# ), apriori_rules['consequents'].str.len())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Apriori Tranction Reduction :  When User Purchase : It can recommend up to\")\n",
    "# item_can_recommend(tranction_redu_apriori_rules['antecedents'].str.len(\n",
    "# ), tranction_redu_apriori_rules['consequents'].str.len())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supports = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "supports = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8,\n",
    "            0.9, 1, 1.1, 1.2, 1.5, 1.6, 1.7, 1.8, 1.9, 2]\n",
    "# plot execution time\n",
    "time_exc_apriori = {}\n",
    "time_exc_tranction_redc = {}\n",
    "for support in supports:\n",
    "    fre_apriori_itemsets, execution_time_apriori = apriori(\n",
    "        item_sets, min_support=support, use_colnames=True)\n",
    "    fre_apriori_tranction_red_itemsets, execution_time_apriori_tranc_red = apriori_tranction_reduction(\n",
    "        item_sets, min_support=support, use_colnames=True, rc_values=rc_values)\n",
    "    time_exc_apriori.update({\n",
    "        support: execution_time_apriori\n",
    "    })\n",
    "    time_exc_tranction_redc.update({\n",
    "        support: execution_time_apriori_tranc_red\n",
    "    })\n",
    "    # print(\n",
    "    #     f\"execution_time_apriori : {execution_time_apriori}, execution_time_apriori_tranc_red : {execution_time_apriori_tranc_red}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_exc_apriori.values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Support\")\n",
    "plt.ylabel(\"Time (ms)\")\n",
    "plt.plot(time_exc_apriori.keys(), time_exc_apriori.values(),\n",
    "         marker=\"o\", label=\"Apriori\")\n",
    "plt.plot(time_exc_tranction_redc.keys(),\n",
    "         time_exc_tranction_redc.values(), marker=\"o\", label=\"Tranction Reduction\")\n",
    "plt.legend()\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fre_apriori_itemsets, execution_time_apriori = apriori(\n",
    "    item_sets, min_support=1, use_colnames=True)\n",
    "\n",
    "\n",
    "fre_apriori_tranction_red_itemsets, execution_time_apriori_tranc_red = apriori_tranction_reduction(\n",
    "    itemset_reductabc, min_support=1, use_colnames=True, rc_values=rc_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fre_apriori_itemsets, execution_time_apriori = apriori(\n",
    "    item_sets, min_support=0.2, use_colnames=True)\n",
    "fre_apriori_itemsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = association_rules(fre_apriori_itemsets,\n",
    "                          metric='confidence', min_threshold=0.6)\n",
    "rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_can_recommend(rules['antecedents'], rules['consequents'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_sets = itemsett_by_country(\"France\")\n",
    "item_sets = item_sets.iloc[0: 10]\n",
    "item_sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supports = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8,\n",
    "            0.9, 1, 1.1, 1.2, 1.5, 1.6, 1.7, 1.8, 1.9, 2]\n",
    "# plot execution time\n",
    "time_exc_apriori = {}\n",
    "time_exc_tranction_redc = {}\n",
    "for support in supports:\n",
    "    fre_apriori_itemsets, execution_time_apriori = apriori(\n",
    "        item_sets, min_support=support, use_colnames=True)\n",
    "    fre_apriori_tranction_red_itemsets, execution_time_apriori_tranc_red = apriori_tranction_reduction(\n",
    "        item_sets, min_support=support, use_colnames=True, rc_values=rc_values)\n",
    "    time_exc_apriori.update({\n",
    "        support: execution_time_apriori\n",
    "    })\n",
    "    time_exc_tranction_redc.update({\n",
    "        support: execution_time_apriori_tranc_red\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fre_apriori_itemsets, execution_time_apriori = apriori(\n",
    "    item_sets, min_support=0.5, use_colnames=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rules_by_country(country, apriori_itemset, tranction_reduction_itemsets, rc_values, supports, metric='confidence', min_threshold=0.6):\n",
    "    itemset_apriori = {}\n",
    "    itemset_tranction_reduction = {}\n",
    "\n",
    "    apriori_rules = {}\n",
    "    tranction_reduction_rules = {}\n",
    "\n",
    "    time_exc_apriori = {}\n",
    "    time_exc_tranction_redc = {}\n",
    "\n",
    "    item_sets_by_country = itemsett_by_country(country)\n",
    "    item_sets_by_country = item_sets_by_country.iloc[0: 10]\n",
    "\n",
    "    for i in supports:\n",
    "\n",
    "        fre_apriori_itemsets, execution_time_apriori = apriori(\n",
    "            apriori_itemset, min_support=i, use_colnames=True)\n",
    "\n",
    "        fre_apriori_tranction_red_itemsets, execution_time_apriori_tranc_reduction = apriori_tranction_reduction(\n",
    "            tranction_reduction_itemsets, min_support=i, use_colnames=True, rc_values=rc_values)\n",
    "\n",
    "        if not fre_apriori_itemsets['support'].empty:\n",
    "            itemset_apriori.update({\n",
    "                i: fre_apriori_itemsets,\n",
    "            })\n",
    "            apriori_rules.update({\n",
    "                i: association_rules(fre_apriori_itemsets,\n",
    "                                 metric=metric, min_threshold=min_threshold)\n",
    "            })\n",
    "            time_exc_apriori.update({\n",
    "                i: execution_time_apriori\n",
    "            })\n",
    "        if not fre_apriori_tranction_red_itemsets['support'].empty:\n",
    "            itemset_tranction_reduction.update({\n",
    "                i: fre_apriori_tranction_red_itemsets\n",
    "            })\n",
    "            tranction_reduction_rules.update({\n",
    "                i: association_rules(fre_apriori_itemsets,\n",
    "                                     metric=metric, min_threshold=min_threshold)\n",
    "            })\n",
    "\n",
    "            time_exc_tranction_redc.update({\n",
    "                i: execution_time_apriori_tranc_reduction\n",
    "            })\n",
    "\n",
    "    return itemset_apriori, itemset_tranction_reduction,  apriori_rules, tranction_reduction_rules, time_exc_apriori, time_exc_tranction_redc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fre_apriori_tranction_red_itemsets, execution_time_apriori_tranc_reduction = apriori_tranction_reduction(\n",
    "    item_sets, min_support=0.2, use_colnames=True, rc_values=rc_values)\n",
    "fre_apriori_tranction_red_itemsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties = sample_dataset['Country'].unique()\n",
    "counties[:4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_by_every_countries(counties, apriori_itemset, tranction_reduction_itemsets, rc_values,  supports):\n",
    "    country_itemset_apriori = {}\n",
    "    country_itemset_apriori_tranction_reduction = {}\n",
    "\n",
    "    country_time_apriori = {}\n",
    "    country_time_apriori__tranction_reduction = {}\n",
    "\n",
    "    country_rule_apriori = {}\n",
    "    country_rule_apriori_tranction_reduction = {}\n",
    "\n",
    "    for i in counties:\n",
    "        itemset_apriori, itemset_tranction_reduction, apriori_rules, tranction_reduction_rules, execution_time_apriori, execution_time_apriori_tranc_reduction = rules_by_country(\n",
    "            i, apriori_itemset, tranction_reduction_itemsets, rc_values,  supports)\n",
    "        country_itemset_apriori.update({\n",
    "            i: itemset_apriori\n",
    "        })\n",
    "        country_itemset_apriori_tranction_reduction.update({\n",
    "            i: itemset_tranction_reduction\n",
    "        })\n",
    "        country_rule_apriori.update({\n",
    "            i: apriori_rules\n",
    "        })\n",
    "        country_rule_apriori_tranction_reduction.update({\n",
    "            i: tranction_reduction_rules\n",
    "        })\n",
    "        country_time_apriori.update({\n",
    "            i: execution_time_apriori\n",
    "        })\n",
    "        country_time_apriori__tranction_reduction.update({\n",
    "            i: execution_time_apriori_tranc_reduction\n",
    "        })\n",
    "\n",
    "    return country_itemset_apriori, country_itemset_apriori_tranction_reduction, country_rule_apriori, country_rule_apriori_tranction_reduction, country_time_apriori, country_time_apriori__tranction_reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot support and Rule It can produce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_by_country('France', item_sets, itemset_reductabc,\n",
    "                 rc_values=rc_values, supports=supports)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this support, how many rule it can generate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unitedKingdom = rule_by_every_countries(\n",
    "    counties[:3],  apriori_itemset=item_sets, tranction_reduction_itemsets=itemset_reductabc, rc_values=rc_values, supports=supports)\n",
    "unitedKingdom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTimeWithSupportEachCountries(rule_by_each_countries, counties):\n",
    "    fig, ax = plt.subplots(1, len(counties), figsize=(10, 5))\n",
    "    fig, bx = plt.subplots(1, len(counties), figsize=(10, 5))\n",
    "    ax = ax.ravel()\n",
    "    bx = bx.ravel()\n",
    "    for i in range(0, len(counties)):\n",
    "        country = counties[i]\n",
    "\n",
    "        apriori_supports_itemset = rule_by_each_countries[0][country]\n",
    "        apriori_tranction_reduction_supports_itemset = rule_by_each_countries[1][country]\n",
    "\n",
    "        apriori_rules = rule_by_each_countries[2][country]\n",
    "        apriori_tranction_reduction_rules = rule_by_each_countries[3][country]\n",
    "\n",
    "        apriori_supports_exc_time = rule_by_each_countries[4][country]\n",
    "        apriori_tranction_reduction_supports_exc_time = rule_by_each_countries[5][country]\n",
    "        print(apriori_supports_exc_time.keys())\n",
    "        ax[i].plot(\n",
    "            apriori_supports_exc_time.keys(), apriori_supports_exc_time.values(), label=\"Apriori\")\n",
    "        ax[i].legend(loc=\"best\")\n",
    "\n",
    "        ax[i].title(\"Hello\")\n",
    "        ax[i].plot(\n",
    "            apriori_tranction_reduction_supports_exc_time.keys(), apriori_tranction_reduction_supports_exc_time.values(), label=\"Tranction Reduction\")\n",
    "        ax[i].set_title(f\"Apriori-{country}\")\n",
    "        ax[i].set_xlabel(\"Support\")\n",
    "        ax[i].set_ylabel(\"Time(s)\")\n",
    "        ax[i].legend(loc=\"best\")\n",
    "        \n",
    "        # bx[i].plot(\n",
    "        #     apriori_tranction_reduction_supports_exc_time.keys(), apriori_tranction_reduction_supports_exc_time.values(), )\n",
    "        # bx[i].set_title(f\"Tranction-Reduction-{country}\")\n",
    "        # bx[i].set_xlabel(\"Support\")\n",
    "        # bx[i].set_ylabel(\"Time(s)\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plotTimeWithSupportEachCountries(unitedKingdom, counties[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unitedKingdom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 10)\n",
    "fig, ax = plt.subplots(1, 3, figsize=(10, 5))\n",
    "ax = ax.ravel()\n",
    "for i in range(3):\n",
    "    y = np.sin(x+i)\n",
    "    ax[i].plot(x, y)\n",
    "    ax[i].set_title(f\"{i}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_with_time = [{0.2: 0.0336321360009606, 0.3: 0.02212243100075284, 0.4: 0.010942570999759482, 0.5: 0.0036832910009252373, 0.6: 0.00968838699918706,\n",
    "                      0.7: 0.009691577000921825, 0.8: 0.0073171189997083275, 0.9: 0.010909566000918858, 1: 0.009190041000692872},\n",
    "                     {0.2: 0.04421744100000069, 0.3: 0.050529127000118024, 0.4: 0.014789639999435167, 0.5: 0.008559862999391044,\n",
    "                      0.6: 0.007137236001653946, 0.7: 0.013940286000433844, 0.8: 0.012399811999785015, 0.9: 0.01900387900059286, 1: 0.01578570700075943}\n",
    "                     ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "x = np.linspace(0, 10)\n",
    "fig, ax = plt.subplots(1, 3, figsize=(10, 5))\n",
    "ax = ax.ravel()\n",
    "for i in range(3):\n",
    "    y = np.sin(x+i)\n",
    "    ax[i].plot(x, y)\n",
    "    ax[i].set_title(f\"{i}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unitedKingdom[0]['France']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "france = rule_by_every_countries([counties[2]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New DataSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/sokhorn/sokhorn/dataSet/groceries/Groceries_dataset.csv\"\n",
    "df = pd.read_csv(path)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grocery_itemsets = (\n",
    "    df.groupby(\n",
    "        ['Member_number', 'itemDescription', ])['Quantity']\n",
    "    .sum().unstack().reset_index().fillna(0)\n",
    "    .set_index(\"Member_number\")\n",
    ")\n",
    "grocery_itemsets = grocery_itemsets.applymap(lambda x: 1 if x > 0 else 0)\n",
    "grocery_itemsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemsets, grocery_apriori_time = apriori(\n",
    "    grocery_itemsets, min_support=0.05, use_colnames=True)\n",
    "itemsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = association_rules(itemsets, metric='confidence', min_threshold=0.3)\n",
    "rules\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
